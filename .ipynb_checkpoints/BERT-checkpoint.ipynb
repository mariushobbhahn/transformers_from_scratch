{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "7600\n"
     ]
    }
   ],
   "source": [
    "dataset_train, dataset_test = torchtext.datasets.AG_NEWS()\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = torchtext.datasets.AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), min_freq=2,\n",
    "                                  specials=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[MASK]\"])\n",
    "vocab.set_default_index(vocab[\"[UNK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53130\n"
     ]
    }
   ],
   "source": [
    "#vocab(['here', 'is', 'an', 'example'])\n",
    "vocab([\"the\"])\n",
    "full_vocab = vocab.vocab.get_stoi().keys()\n",
    "print(len(full_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([5.2000e+01, 6.6170e+03, 4.7691e+04, 3.0000e+00, 1.0000e+01,\n",
      "       1.0000e+00, 6.0000e+00, 4.8000e+01, 7.0000e+01, 7.4700e+02,\n",
      "       3.0000e+00, 4.0000e+00, 2.6377e+04, 7.0000e+00, 2.2500e+02,\n",
      "       3.3000e+01, 1.8870e+03, 1.3152e+04, 3.0000e+00, 1.0000e+01,\n",
      "       1.0000e+00, 6.0000e+00, 5.2000e+01, 1.3610e+03, 5.0000e+00,\n",
      "       5.0000e+01, 7.9570e+03, 7.0000e+00, 2.2500e+02, 6.4000e+01,\n",
      "       1.0000e+01, 6.4220e+03, 3.0000e+00, 1.1000e+01, 1.0000e+00,\n",
      "       2.3200e+02, 3.0000e+00, 4.8000e+01, 5.0000e+00, 1.3152e+04,\n",
      "       3.8000e+01, 1.5491e+04, 1.0000e+01, 3.1800e+02, 4.0000e+00,\n",
      "       1.0000e+01, 3.0000e+00, 3.0441e+04, 6.0000e+00, 5.2000e+01]), array([5.2000e+01, 6.6170e+03, 4.7691e+04, 2.6430e+03, 1.0000e+01,\n",
      "       1.0000e+00, 6.0000e+00, 4.8000e+01, 7.0000e+01, 7.4700e+02,\n",
      "       1.0618e+04, 4.0000e+00, 4.3080e+03, 7.0000e+00, 2.2500e+02,\n",
      "       3.3000e+01, 1.8870e+03, 1.3152e+04, 6.0000e+00, 1.0000e+01,\n",
      "       1.0000e+00, 6.0000e+00, 5.2000e+01, 1.3610e+03, 5.0000e+00,\n",
      "       5.0000e+01, 7.9570e+03, 7.0000e+00, 2.2500e+02, 6.4000e+01,\n",
      "       1.0000e+01, 6.4220e+03, 6.0000e+00, 1.1000e+01, 1.0000e+00,\n",
      "       2.3200e+02, 6.0000e+00, 4.8000e+01, 5.0000e+00, 1.3152e+04,\n",
      "       3.8000e+01, 1.5491e+04, 1.0000e+01, 3.1800e+02, 4.0000e+00,\n",
      "       1.0000e+01, 1.9190e+03, 3.0441e+04, 6.0000e+00, 5.2000e+01]), array([False, False,  True,  True, False, False, False, False, False,\n",
      "       False,  True, False,  True, False, False, False, False, False,\n",
      "        True, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False,  True, False, False, False,\n",
      "        True, False, False, False, False, False, False, False, False,\n",
      "       False,  True, False, False, False]))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# copied from tutorial, added padding\n",
    "def text_pipeline(x, max_len, percentage_masked=0.15):\n",
    "    vocab_list = np.array(vocab(tokenizer(x)))[:max_len]\n",
    "    k = len(vocab_list)\n",
    "    missing_len = max_len - k\n",
    "    missing_list = missing_len * vocab([\"[PAD]\"])\n",
    "    # true labels\n",
    "    labels = np.concatenate([copy.deepcopy(vocab_list), missing_list])\n",
    "    # padding mask\n",
    "    mask = np.random.choice([True, False], k, p=[percentage_masked, 1-percentage_masked])\n",
    "    #print(mask)\n",
    "    mask_idxs = np.arange(len(vocab_list))[mask]\n",
    "    random.shuffle(mask_idxs)\n",
    "    mask_80 = mask_idxs[:math.floor(0.8*len(mask_idxs))]\n",
    "    mask_10 = mask_idxs[math.floor(0.8*len(mask_idxs)):math.floor(0.9*len(mask_idxs))]\n",
    "    #print(mask_idxs)\n",
    "    #print(mask_80)\n",
    "    #print(mask_10)\n",
    "    vocab_list[mask_80] = vocab([\"[MASK]\"])\n",
    "    # fill remaining 10 percent with random words\n",
    "    random_words = np.random.choice(list(full_vocab), len(mask_10), replace=True)\n",
    "    vocab_list[mask_10] = vocab(list(random_words))\n",
    "    \n",
    "    return(np.concatenate([vocab_list, missing_list]), labels, np.concatenate([mask, np.zeros(missing_len, dtype=bool)]))\n",
    "\n",
    "print(text_pipeline('He married Mabel Scott in 1890, but they soon separated. Unable to get an English divorce, in 1900, he became the first celebrity to get one in Nevada, and remarried there, but the divorce was invalid in England. In June 1901, he was arrested for bigamy, and was convicted before the House of Lords, the last time a peer was convicted by the Lords.', 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# copied from tutorial, removed offsets\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, mask_list = [], [], []\n",
    "    for _, text in batch:\n",
    "        input_, label_, mask_ = text_pipeline(text, max_len=100, percentage_masked=0.15)\n",
    "        text_list.append(torch.tensor(input_, dtype=torch.int64))\n",
    "        label_list.append(label_)\n",
    "        mask_list.append(torch.tensor(mask_))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.cat(text_list).view(len(label_list), -1)\n",
    "    mask_list = torch.cat(mask_list).view(len(label_list), -1)\n",
    "    return text_list.to(device), label_list.to(device), mask_list\n",
    "\n",
    "train_iter = dataset_train\n",
    "BATCH_SIZE = 32\n",
    "dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build classifier transformer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyBERT(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, heads, seq_length, vocab_size, depth=5, num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_emb = nn.Embedding(seq_length, embedding_dim)\n",
    "        self.num_heads = heads\n",
    "\n",
    "        # sequence of transformers\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
    "                                                            nhead=self.num_heads, \n",
    "                                                            batch_first=True, dropout=0.1))\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "        \n",
    "        # final linear layer\n",
    "        self.last_linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask_idxs):\n",
    "        # generate token embeddings\n",
    "        tokens = self.token_emb(x)\n",
    "        batch_size, token_size, embed_size = tokens.size()\n",
    "\n",
    "        # generate position embeddings\n",
    "        positions = torch.arange(token_size)\n",
    "        positions = self.pos_emb(positions).expand(batch_size, token_size, embed_size)\n",
    "\n",
    "        x = tokens + positions\n",
    "        x = self.tblocks(x)\n",
    "\n",
    "        # only predict on the masked tokens\n",
    "        masked_idxs = mask_idxs[:, :, None].expand(-1, -1, x.size(-1))\n",
    "        #print(\"x size: \", x.size())\n",
    "        #print(\"mask size: \", mask_idxs.size())\n",
    "        masked_tokens = x[mask_idxs] #torch.gather(x, dim=1, index=masked_pos)\n",
    "        out = self.last_linear(masked_tokens)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bert = MyBERT(embedding_dim=30, heads=5, \n",
    "                        seq_length=100, vocab_size=len(full_vocab),\n",
    "                        depth=1)\n",
    "\n",
    "optimizer = torch.optim.Adam(my_bert.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7164b7a94b9b4d84a96f20ea48c7b4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3750.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-dd7d5fc0a30c>:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
      "<ipython-input-7-dd7d5fc0a30c>:13: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  label_list = torch.tensor(label_list, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a010a0df0b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtraining_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch: \", epoch)\n",
    "    training_loss = 0\n",
    "    for i, (x, y, mask_idxs) in tqdm(enumerate(dataloader), total=len(dataset_train)//BATCH_SIZE):\n",
    "\n",
    "        #print(x)\n",
    "        if i > 50: break\n",
    "        #mask_idxs = mask_idxs\n",
    "        #print(mask_idxs.size())\n",
    "        optimizer.zero_grad()\n",
    "        #print(y.size())\n",
    "        out = my_bert(x, mask_idxs)\n",
    "        \n",
    "        masked_idxs = mask_idxs[:, :, None].expand(-1, -1, y.size(-1))\n",
    "#        print(\"x size: \", x.size())\n",
    "#        print(\"mask size: \", mask_idxs.size())\n",
    "        y_masked = y[mask_idxs]\n",
    "        #y_masked = torch.gather(y, dim=1, index=mask_idxs)\n",
    "#        loss = criterion(out.transpose(1,2), y_masked)\n",
    "        loss = criterion(out, y_masked)\n",
    "        training_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"training_loss: \", training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "BERT_PATH = \"parameters/my_bert_small.pth\"\n",
    "print(\"saving model at: {}\".format(BERT_PATH))\n",
    "torch.save(my_bert.state_dict(), BERT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "BERT_PATH = \"parameters/my_bert_small.pth\"\n",
    "my_bert = MyBERT(embedding_dim=30, heads=5, \n",
    "                        seq_length=100, vocab_size=len(full_vocab),\n",
    "                        depth=1)\n",
    "\n",
    "print(\"loading model from: {}\".format(BERT_PATH))\n",
    "CIFAR10_model.load_state_dict(torch.load(BERT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
