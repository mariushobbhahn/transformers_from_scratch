{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Understanding attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build our own attention function, single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write our own attention function; single head\n",
    "\n",
    "\"\"\"\n",
    "input: \n",
    "- matrix X, size: num_words_in_input x embedding_dim\n",
    "- matrix W_Q, size: embedding_dim x embedding_dim \n",
    "- matrix W_K, size: embedding_dim x embedding_dim \n",
    "- matrix W_V, size: embedding_dim x embedding_dim \n",
    "output: \n",
    "- matrix Z, size: num_words_in_input x embedding_dim\n",
    "\"\"\"\n",
    "\n",
    "def my_attention(X, W_Q, W_K, W_V):\n",
    "    \n",
    "    # get query, key, value\n",
    "    Q = X @ W_Q\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "    d_k = torch.tensor(K.size(-1))\n",
    "    \n",
    "    # get attention\n",
    "    Z = nn.functional.softmax((Q @ K.T)/torch.sqrt(d_k), dim=1) @ V\n",
    "    \n",
    "    return(Z, Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "### do the shapes add up?\n",
    "num_words_in_input_dim = 8\n",
    "embedding_dim = 4\n",
    "\n",
    "X_test = torch.rand((num_words_in_input_dim, embedding_dim))\n",
    "W_Q_test = torch.rand((embedding_dim, embedding_dim))\n",
    "W_K_test = torch.rand((embedding_dim, embedding_dim))\n",
    "W_V_test = torch.rand((embedding_dim, embedding_dim))\n",
    "\n",
    "# run through\n",
    "Z_test, Q_test, K_test, V_test = my_attention(X_test, W_Q_test, W_K_test, W_V_test)\n",
    "print(Z_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4])\n",
      "tensor([[[1.2124, 1.1662, 1.0024, 1.0880],\n",
      "         [1.0774, 1.0575, 0.8936, 0.9834],\n",
      "         [1.0694, 1.0527, 0.8875, 0.9782],\n",
      "         [1.0644, 1.0506, 0.8834, 0.9755],\n",
      "         [1.0938, 1.0761, 0.9070, 1.0000],\n",
      "         [1.1007, 1.0772, 0.9124, 1.0024],\n",
      "         [1.0363, 1.0285, 0.8611, 0.9533],\n",
      "         [1.0900, 1.0684, 0.9034, 0.9937]]])\n",
      "tensor([[1.2124, 1.1662, 1.0024, 1.0880],\n",
      "        [1.0774, 1.0575, 0.8936, 0.9834],\n",
      "        [1.0694, 1.0527, 0.8875, 0.9782],\n",
      "        [1.0644, 1.0506, 0.8834, 0.9755],\n",
      "        [1.0938, 1.0761, 0.9070, 1.0000],\n",
      "        [1.1007, 1.0772, 0.9124, 1.0024],\n",
      "        [1.0363, 1.0285, 0.8611, 0.9533],\n",
      "        [1.0900, 1.0684, 0.9034, 0.9937]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "### compare with the PyTorch function for single attention head\n",
    "Z_test_PT, _ = nn.functional._scaled_dot_product_attention(\n",
    "                           Q_test.view(1, num_words_in_input_dim, embedding_dim),\n",
    "                           K_test.view(1, num_words_in_input_dim, embedding_dim),\n",
    "                           V_test.view(1, num_words_in_input_dim, embedding_dim),\n",
    "                    attn_mask=None, dropout_p=0.)\n",
    "print(Z_test_PT.size())\n",
    "print(Z_test_PT)\n",
    "print(Z_test)\n",
    "print(torch.allclose(Z_test_PT, Z_test))\n",
    "\n",
    "### WIN -> allowed to replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build one attention block - single head\n",
    "\n",
    "by block we mean the attention function with weights + linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build a NN module for our attention mechanism\n",
    "\n",
    "class MySingleheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, W_linear, b_linear):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_linear = W_linear\n",
    "        self.b_linear = b_linear\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        \n",
    "        Z_test_PT, _ = nn.functional._scaled_dot_product_attention(\n",
    "                           Q.view(1, num_words_in_input_dim, embedding_dim),\n",
    "                           K.view(1, num_words_in_input_dim, embedding_dim),\n",
    "                           V.view(1, num_words_in_input_dim, embedding_dim),\n",
    "                    attn_mask=None, dropout_p=0.)\n",
    "        \n",
    "        # Linear layer\n",
    "        out = Z_test_PT @ self.W_linear.T + self.b_linear\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 4])\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# this includes the linear layer after the attention head\n",
    "# and therefore yields different results\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1, batch_first=True, bias=False)\n",
    "out_test_PT, _ = multihead_attn(X_test.view(1, num_words_in_input_dim, embedding_dim),\n",
    "                           X_test.view(1, num_words_in_input_dim, embedding_dim),\n",
    "                           X_test.view(1, num_words_in_input_dim, embedding_dim),\n",
    "                            need_weights=False)\n",
    "\n",
    "in_proj_weight_attn = multihead_attn.in_proj_weight\n",
    "print(in_proj_weight_attn.size())\n",
    "in_proj_weight_Q = in_proj_weight_attn[:embedding_dim,:]\n",
    "in_proj_weight_K = in_proj_weight_attn[embedding_dim:2*embedding_dim,:]\n",
    "in_proj_weight_V = in_proj_weight_attn[2*embedding_dim:3*embedding_dim,:]\n",
    "\n",
    "W_Linear_test = multihead_attn.out_proj.weight\n",
    "#b_Linear_test = multihead_attn.out_proj.bias\n",
    "print(W_Linear_test.size())\n",
    "#print(b_Linear_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1113, -0.0103, -0.3275, -0.2489],\n",
      "         [ 0.1111, -0.0108, -0.3256, -0.2471],\n",
      "         [ 0.1117, -0.0110, -0.3273, -0.2485],\n",
      "         [ 0.1101, -0.0094, -0.3244, -0.2465],\n",
      "         [ 0.1110, -0.0101, -0.3263, -0.2478],\n",
      "         [ 0.1117, -0.0111, -0.3273, -0.2484],\n",
      "         [ 0.1101, -0.0093, -0.3245, -0.2465],\n",
      "         [ 0.1099, -0.0101, -0.3226, -0.2447]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[ 0.1113, -0.0103, -0.3275, -0.2489],\n",
      "         [ 0.1111, -0.0108, -0.3256, -0.2471],\n",
      "         [ 0.1117, -0.0110, -0.3273, -0.2485],\n",
      "         [ 0.1101, -0.0094, -0.3244, -0.2465],\n",
      "         [ 0.1110, -0.0101, -0.3263, -0.2478],\n",
      "         [ 0.1117, -0.0111, -0.3273, -0.2484],\n",
      "         [ 0.1101, -0.0093, -0.3245, -0.2465],\n",
      "         [ 0.1099, -0.0101, -0.3226, -0.2447]]], grad_fn=<TransposeBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "single_head_attention_test = MySingleheadAttention(W_Linear_test, torch.zeros((embedding_dim,)))\n",
    "#regenerate Q_test, K_test, V_test with the weights used by the pytorch function\n",
    "Z_test, Q_test, K_test, V_test = my_attention(X_test, in_proj_weight_Q.T, in_proj_weight_K.T, in_proj_weight_V.T)\n",
    "#Note the transpose of W_Q, W_K and W_V due to the way linear layers work in PyTorch\n",
    "out_test = single_head_attention_test(Q_test, K_test, V_test)\n",
    "print(out_test)\n",
    "print(out_test_PT)\n",
    "print(torch.allclose(out_test, out_test_PT))\n",
    "# it results in the same thing -> WIN -> allowed to replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build one attention block - multihead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build a NN module for our multihead attention mechanism\n",
    "\n",
    "class MyMultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, W_linear, b_linear, num_heads=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_linear = W_linear\n",
    "        self.b_linear = b_linear\n",
    "        self.num_heads=num_heads\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # chunk Q, K, V in num_heads along embedding size\n",
    "        Q_chunks = Q.chunk(self.num_heads, dim=-1)\n",
    "        K_chunks = K.chunk(self.num_heads, dim=-1)\n",
    "        V_chunks = V.chunk(self.num_heads, dim=-1)\n",
    "        \n",
    "        # apply attention to each chunk\n",
    "        Z_test_list = []\n",
    "        for i in range(len(Q_chunks)):\n",
    "            Q_head = Q_chunks[i]\n",
    "            K_head = K_chunks[i]\n",
    "            V_head = V_chunks[i]\n",
    "            Z_test_PT_, _ = nn.functional._scaled_dot_product_attention(\n",
    "                           Q_head.view(1, num_words_in_input_dim, embedding_dim//self.num_heads),\n",
    "                           K_head.view(1, num_words_in_input_dim, embedding_dim//self.num_heads),\n",
    "                           V_head.view(1, num_words_in_input_dim, embedding_dim//self.num_heads),\n",
    "                            attn_mask=None, dropout_p=0.)\n",
    "            Z_test_list.append(Z_test_PT_)\n",
    "        \n",
    "        # concatenate all chunks along embedding size\n",
    "        Z_test_PT = torch.cat(Z_test_list, dim=-1)\n",
    "        \n",
    "        # Linear layer\n",
    "        out = Z_test_PT @ self.W_linear.T + self.b_linear\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 12])\n",
      "torch.Size([12, 12])\n",
      "torch.Size([1, 8, 12])\n"
     ]
    }
   ],
   "source": [
    "### compare with the PyTorch function for multiple attention heads\n",
    "\n",
    "# PyTorch doc:\n",
    "# num_heads – Number of parallel attention heads. Note that embed_dim will be split\n",
    "# across num_heads (i.e. each head will have dimension embed_dim // num_heads\n",
    "\n",
    "# set embed_dim to 12 to not confuse the number of attention heads with embed dim\n",
    "embedding_dim=12\n",
    "\n",
    "# this includes the linear layer after the attention head\n",
    "# and therefore yields different results\n",
    "X_test = torch.rand((num_words_in_input_dim, embedding_dim))\n",
    "num_attention_heads = 4\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                             num_heads=num_attention_heads, batch_first=True, bias=False)\n",
    "X_test_heads = X_test.view(1, num_words_in_input_dim, embedding_dim)\n",
    "\n",
    "out_test_PT, _ = multihead_attn(X_test_heads, X_test_heads, X_test_heads,\n",
    "                            need_weights=False)\n",
    "\n",
    "in_proj_weight_attn = multihead_attn.in_proj_weight\n",
    "print(in_proj_weight_attn.size())\n",
    "in_proj_weight_Q = in_proj_weight_attn[:embedding_dim,:]\n",
    "in_proj_weight_K = in_proj_weight_attn[embedding_dim:2*embedding_dim,:]\n",
    "in_proj_weight_V = in_proj_weight_attn[2*embedding_dim:3*embedding_dim,:]\n",
    "\n",
    "W_Linear_test = multihead_attn.out_proj.weight\n",
    "#b_Linear_test = multihead_attn.out_proj.bias\n",
    "print(W_Linear_test.size())\n",
    "#print(b_Linear_test.size())\n",
    "print(out_test_PT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2658, -0.1326, -0.0226,  0.3819, -0.2050, -0.1240, -0.0373,\n",
      "           0.2997, -0.4118, -0.3684, -0.2174,  0.3167],\n",
      "         [ 0.2661, -0.1293, -0.0220,  0.3800, -0.2064, -0.1232, -0.0362,\n",
      "           0.2978, -0.4111, -0.3658, -0.2163,  0.3180],\n",
      "         [ 0.2662, -0.1279, -0.0221,  0.3793, -0.2056, -0.1232, -0.0364,\n",
      "           0.2974, -0.4100, -0.3645, -0.2167,  0.3193],\n",
      "         [ 0.2659, -0.1314, -0.0231,  0.3828, -0.2068, -0.1230, -0.0362,\n",
      "           0.2986, -0.4123, -0.3678, -0.2179,  0.3173],\n",
      "         [ 0.2654, -0.1282, -0.0220,  0.3808, -0.2047, -0.1249, -0.0354,\n",
      "           0.2993, -0.4129, -0.3650, -0.2172,  0.3199],\n",
      "         [ 0.2664, -0.1292, -0.0210,  0.3815, -0.2046, -0.1247, -0.0369,\n",
      "           0.3000, -0.4126, -0.3658, -0.2181,  0.3207],\n",
      "         [ 0.2657, -0.1299, -0.0229,  0.3779, -0.2068, -0.1225, -0.0375,\n",
      "           0.2971, -0.4092, -0.3659, -0.2143,  0.3171],\n",
      "         [ 0.2642, -0.1309, -0.0221,  0.3813, -0.2017, -0.1266, -0.0352,\n",
      "           0.3014, -0.4139, -0.3668, -0.2178,  0.3165]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[ 0.2658, -0.1326, -0.0226,  0.3819, -0.2050, -0.1240, -0.0373,\n",
      "           0.2997, -0.4118, -0.3684, -0.2174,  0.3167],\n",
      "         [ 0.2661, -0.1293, -0.0220,  0.3800, -0.2064, -0.1232, -0.0362,\n",
      "           0.2978, -0.4111, -0.3658, -0.2163,  0.3180],\n",
      "         [ 0.2662, -0.1279, -0.0221,  0.3793, -0.2056, -0.1232, -0.0364,\n",
      "           0.2974, -0.4100, -0.3645, -0.2167,  0.3193],\n",
      "         [ 0.2659, -0.1314, -0.0231,  0.3828, -0.2068, -0.1230, -0.0362,\n",
      "           0.2986, -0.4123, -0.3678, -0.2179,  0.3173],\n",
      "         [ 0.2654, -0.1282, -0.0220,  0.3808, -0.2047, -0.1249, -0.0354,\n",
      "           0.2993, -0.4129, -0.3650, -0.2172,  0.3199],\n",
      "         [ 0.2664, -0.1292, -0.0210,  0.3815, -0.2046, -0.1247, -0.0369,\n",
      "           0.3000, -0.4126, -0.3658, -0.2181,  0.3207],\n",
      "         [ 0.2657, -0.1299, -0.0229,  0.3779, -0.2068, -0.1225, -0.0375,\n",
      "           0.2971, -0.4092, -0.3659, -0.2143,  0.3171],\n",
      "         [ 0.2642, -0.1309, -0.0221,  0.3813, -0.2017, -0.1266, -0.0352,\n",
      "           0.3014, -0.4139, -0.3668, -0.2178,  0.3165]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "multi_head_attention_test = MyMultiheadAttention(W_Linear_test, torch.zeros((embedding_dim,)),\n",
    "                                                 num_heads=num_attention_heads)\n",
    "#regenerate Q_test, K_test, V_test with the weights used by the pytorch function\n",
    "Z_test, Q_test, K_test, V_test = my_attention(X_test, in_proj_weight_Q.T, in_proj_weight_K.T, in_proj_weight_V.T)\n",
    "#Note the transpose of W_Q, W_K and W_V due to the way linear layers work in PyTorch\n",
    "out_test = multi_head_attention_test(Q_test, K_test, V_test)\n",
    "print(out_test)\n",
    "print(out_test_PT)\n",
    "print(torch.allclose(out_test, out_test_PT))\n",
    "# it results in the same thing -> WIN -> we can now use PyTorch's MultiheadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build one encoder block\n",
    "\n",
    "where one encoder block is a self attention block with residual connections and layer normalization. This is followed by a linear layer with another residual connection and another layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our own encoder Block\n",
    "class MyEncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads=4, embedding_dim=12):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.multi_head_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                             num_heads=num_heads, batch_first=True, bias=False)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        self.linear1 = nn.Linear(embedding_dim, 4*embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(4*embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        attn, _ = self.multi_head_attention(X, X, X, need_weights=False)\n",
    "        n1 = self.norm1(X + attn)\n",
    "        \n",
    "        x = self.linear2(self.relu(self.linear1(n1)))\n",
    "        n2 = self.norm2(x + n1)\n",
    "        return(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 12])\n"
     ]
    }
   ],
   "source": [
    "### test if our model has any errors\n",
    "my_encoder_block = MyEncoderBlock(num_heads=4, embedding_dim=12)\n",
    "X_test_heads = X_test.view(1, num_words_in_input_dim, embedding_dim)\n",
    "print(my_encoder_block(X_test_heads).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 12])\n"
     ]
    }
   ],
   "source": [
    "### compare to PyTorch implementation\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_attention_heads, \n",
    "                                                 batch_first=True, dropout=0.)\n",
    "out_test_PT = encoder_layer(X_test_heads)\n",
    "print(out_test_PT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### comparison by visual inspection of the PyTorch implementation: https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer\n",
    "# we conclude it's good enough; actually setting all the weights manually would be too much of a pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
