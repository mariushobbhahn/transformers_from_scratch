{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT from scratch\n",
    "\n",
    "We build a BERT model from scratch. We use the AG_NEWS dataset that is built-in in torchtext and use some of the tokenization tools from torchtext. A huggingface pipeline might have taken care of all of the pre-training steps but we wanted to get a more detailed understanding of the entire pipeline. \n",
    "\n",
    "We train a small-ish model for 5 epochs and test it in two ways. First, we look at its predictions on the first test batch to see if the predictions are plausible. Secondly, we investigate the test loss on a random network vs. our trained network. \n",
    "\n",
    "Our model is not perfect but we think it is sufficiently different from random chance that we can say it has learned something and our pipeline is functional. Our goal was not to reproduce or beat the state of the art but just to built a working pipeline so we stop there. \n",
    "\n",
    "Interestingly, a larger model learned to just predict the \"[PAD]\" token whereas a smaller model predicted mostly \",\", \".\", \"a\", \"the\", and the likes. Not sure where this comes from. Maybe we'd have to train it for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "7600\n"
     ]
    }
   ],
   "source": [
    "dataset_train, dataset_test = torchtext.datasets.AG_NEWS()\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = torchtext.datasets.AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), min_freq=2,\n",
    "                                  specials=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[MASK]\"])\n",
    "vocab.set_default_index(vocab[\"[UNK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53130\n"
     ]
    }
   ],
   "source": [
    "#vocab(['here', 'is', 'an', 'example'])\n",
    "vocab([\"the\"])\n",
    "full_vocab = vocab.vocab.get_stoi().keys()\n",
    "print(len(full_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([5.2000e+01, 6.6170e+03, 4.7691e+04, 2.6430e+03, 1.0000e+01,\n",
      "       1.0000e+00, 6.0000e+00, 4.8000e+01, 7.0000e+01, 7.4700e+02,\n",
      "       3.0000e+00, 4.0000e+00, 3.0000e+00, 7.0000e+00, 2.2500e+02,\n",
      "       3.0000e+00, 1.8870e+03, 1.3152e+04, 6.0000e+00, 1.0000e+01,\n",
      "       1.0000e+00, 6.0000e+00, 5.2000e+01, 1.3610e+03, 5.0000e+00,\n",
      "       5.0000e+01, 7.9570e+03, 7.0000e+00, 2.2500e+02, 6.4000e+01,\n",
      "       1.0000e+01, 6.4220e+03, 6.0000e+00, 1.1000e+01, 1.0000e+00,\n",
      "       2.3200e+02, 6.0000e+00, 4.8000e+01, 5.0000e+00, 1.3152e+04,\n",
      "       3.8000e+01, 1.5491e+04, 3.0000e+00, 3.1800e+02, 4.0000e+00,\n",
      "       1.0000e+01, 1.9190e+03, 3.0441e+04, 6.0000e+00, 5.2000e+01]), array([5.2000e+01, 6.6170e+03, 4.7691e+04, 2.6430e+03, 1.0000e+01,\n",
      "       1.0000e+00, 6.0000e+00, 4.8000e+01, 7.0000e+01, 7.4700e+02,\n",
      "       1.0618e+04, 4.0000e+00, 4.3080e+03, 7.0000e+00, 2.2500e+02,\n",
      "       3.3000e+01, 1.8870e+03, 1.3152e+04, 6.0000e+00, 1.0000e+01,\n",
      "       1.0000e+00, 6.0000e+00, 5.2000e+01, 1.3610e+03, 5.0000e+00,\n",
      "       5.0000e+01, 7.9570e+03, 7.0000e+00, 2.2500e+02, 6.4000e+01,\n",
      "       1.0000e+01, 6.4220e+03, 6.0000e+00, 1.1000e+01, 1.0000e+00,\n",
      "       2.3200e+02, 6.0000e+00, 4.8000e+01, 5.0000e+00, 1.3152e+04,\n",
      "       3.8000e+01, 1.5491e+04, 1.0000e+01, 3.1800e+02, 4.0000e+00,\n",
      "       1.0000e+01, 1.9190e+03, 3.0441e+04, 6.0000e+00, 5.2000e+01]), array([False, False, False, False, False, False, False, False, False,\n",
      "       False,  True, False,  True, False, False,  True, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False,  True, False, False, False, False,\n",
      "       False, False, False, False, False, False,  True, False, False,\n",
      "       False, False, False, False, False]))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# copied from tutorial, added padding\n",
    "def text_pipeline(x, max_len, percentage_masked=0.15):\n",
    "    vocab_list = np.array(vocab(tokenizer(x)))[:max_len]\n",
    "    k = len(vocab_list)\n",
    "    missing_len = max_len - k\n",
    "    missing_list = missing_len * vocab([\"[PAD]\"])\n",
    "    # true labels\n",
    "    labels = np.concatenate([copy.deepcopy(vocab_list), missing_list])\n",
    "    # padding mask\n",
    "    mask = np.random.choice([True, False], k, p=[percentage_masked, 1-percentage_masked])\n",
    "    mask_idxs = np.arange(len(vocab_list))[mask]\n",
    "    random.shuffle(mask_idxs)\n",
    "    mask_80 = mask_idxs[:math.floor(0.8*len(mask_idxs))]\n",
    "    mask_10 = mask_idxs[math.floor(0.8*len(mask_idxs)):math.floor(0.9*len(mask_idxs))]\n",
    "    vocab_list[mask_80] = vocab([\"[MASK]\"])\n",
    "    # fill remaining 10 percent with random words\n",
    "    random_words = np.random.choice(list(full_vocab), len(mask_10), replace=True)\n",
    "    vocab_list[mask_10] = vocab(list(random_words))\n",
    "    \n",
    "    return(np.concatenate([vocab_list, missing_list]), labels, np.concatenate([mask, np.zeros(missing_len, dtype=bool)]))\n",
    "\n",
    "print(text_pipeline('He married Mabel Scott in 1890, but they soon separated. Unable to get an English divorce, in 1900, he became the first celebrity to get one in Nevada, and remarried there, but the divorce was invalid in England. In June 1901, he was arrested for bigamy, and was convicted before the House of Lords, the last time a peer was convicted by the Lords.', 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# copied from tutorial, removed offsets\n",
    "from torch.utils.data import DataLoader\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, mask_list = [], [], []\n",
    "    for _, text in batch:\n",
    "        input_, label_, mask_ = text_pipeline(text, max_len=100, percentage_masked=0.15)\n",
    "        text_list.append(torch.tensor(input_, dtype=torch.int64))\n",
    "        label_list.append(label_)\n",
    "        mask_list.append(torch.tensor(mask_))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.cat(text_list).view(len(label_list), -1)\n",
    "    mask_list = torch.cat(mask_list).view(len(label_list), -1)\n",
    "    return text_list.to(DEVICE), label_list.to(DEVICE), mask_list\n",
    "\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "train_iter = to_map_style_dataset(dataset_train)  #Map-style dataset\n",
    "BATCH_SIZE = 32\n",
    "dataloader = DataLoader(list(train_iter), batch_size=BATCH_SIZE, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build classifier transformer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyBERT(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, heads, seq_length, vocab_size, device, depth=5, num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_emb = nn.Embedding(seq_length, embedding_dim)\n",
    "        self.num_heads = heads\n",
    "        self.device = device\n",
    "\n",
    "        # sequence of transformers\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
    "                                                            nhead=self.num_heads, \n",
    "                                                            batch_first=True, dropout=0.1))\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "        \n",
    "        # final linear layer\n",
    "        self.last_linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask_idxs):\n",
    "        # generate token embeddings\n",
    "        tokens = self.token_emb(x)\n",
    "        batch_size, token_size, embed_size = tokens.size()\n",
    "\n",
    "        # generate position embeddings\n",
    "        positions = torch.arange(token_size).to(self.device)\n",
    "        positions = self.pos_emb(positions).expand(batch_size, token_size, embed_size)\n",
    "\n",
    "        x = tokens + positions\n",
    "        x = self.tblocks(x)\n",
    "\n",
    "        out = self.last_linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-mini: n_layer=6, n_head=6, n_embd=192\n",
    "# gpt-micro: n_layer=4, n_head=4, n_embd=128\n",
    "# gpt-nano: n_layer=3, n_head=3, n_embd=48\n",
    "# gpt-mini2: n_layers=6, n_head=16, n_embd=128\n",
    "# gpt-mini3: n_layers=10, n_head=32, n_embd=256\n",
    "\n",
    "\n",
    "EMBED_DIM = 48\n",
    "NUM_HEADS = 6\n",
    "NUM_LAYERS = 3\n",
    "SEQ_LENGTH = 100\n",
    "VOCAB_SIZE = len(full_vocab)\n",
    "\n",
    "my_bert = MyBERT(embedding_dim=EMBED_DIM,\n",
    "                 heads=NUM_HEADS, \n",
    "                 seq_length=100, \n",
    "                 vocab_size=len(full_vocab),\n",
    "                 device=DEVICE,\n",
    "                 depth=NUM_LAYERS).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(my_bert.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affcb0b98f8d47b9b38e834ff92f96b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8045/53046415.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
      "/tmp/ipykernel_8045/53046415.py:13: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  label_list = torch.tensor(label_list, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:  tensor(3906.4558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch:  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540c4232c96a43d98e88c231c2409e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:  tensor(1912.2423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch:  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e28ac3092c4e91839d0d7c88f03db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:  tensor(1692.7605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch:  3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab5e38f5b8e42e280003f20edcade4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:  tensor(1599.1382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch:  4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3811f7800fbe46f7b81c452eaab79ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:  tensor(1543.2677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch:  5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd962caa04c46c3ba3e14a8ee1f464e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:  tensor(1505.9883, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch:  6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232a9127ff094d6eb644b73b2147cf23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:  tensor(1474.9716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch:  7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f05e918df614c4794d32da07a16ae9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:  tensor(1448.3875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch:  8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bb594b942444a1b6ea930c88016a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:  tensor(1427.6262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch:  9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac3e7288df44a04b14140d2d7bb1d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:  tensor(1408.1722, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch: \", epoch)\n",
    "    training_loss = 0\n",
    "    i = 0\n",
    "    for (x, y, mask_idxs) in tqdm(dataloader):\n",
    "        \n",
    "        i += 1\n",
    "        optimizer.zero_grad()\n",
    "        out = my_bert(x, mask_idxs)\n",
    "        out = torch.swapaxes(out, 1, 2)\n",
    "\n",
    "        loss = criterion(out, y)\n",
    "        training_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"training_loss: \", training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model at: parameters/my_bert_mini3.pth\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "BERT_PATH = \"parameters/my_bert_mini3.pth\"\n",
    "print(\"saving model at: {}\".format(BERT_PATH))\n",
    "torch.save(my_bert.state_dict(), BERT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing selected outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: parameters/my_bert_mini3.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "BERT_PATH = \"parameters/my_bert_mini3.pth\"\n",
    "my_bert = MyBERT(embedding_dim=EMBED_DIM,\n",
    "                 heads=NUM_HEADS, \n",
    "                 seq_length=100, \n",
    "                 vocab_size=len(full_vocab),\n",
    "                 device=DEVICE,\n",
    "                 depth=NUM_LAYERS).to(DEVICE)\n",
    "\n",
    "print(\"loading model from: {}\".format(BERT_PATH))\n",
    "my_bert.load_state_dict(torch.load(BERT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-11.8586,   6.7565, -17.0153,  ...,  -9.8805, -11.0341, -11.1836],\n",
      "         [ -1.5625,   3.8778, -10.2055,  ...,  -6.5444,  -6.1065,  -5.9842],\n",
      "         [-10.5377,   4.4939, -13.0039,  ..., -10.3370,  -6.1239, -12.6618],\n",
      "         ...,\n",
      "         [ -9.7811,   2.6191, -17.0421,  ...,  -7.3072,  -5.5557, -16.9369],\n",
      "         [ -4.1034,   4.1237, -10.6320,  ...,  -7.3408,  -5.9452,  -4.7843],\n",
      "         [ -8.0328,   4.1775, -16.4952,  ...,  -9.2391, -11.7229,  -1.8939]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "he ,_married mabel ,_scott in [UNK] ,_, but they soon separated ,_. unable_unable to get an english nips_divorce , in [UNK] , he became the first celebrity to get one in nevada , and [UNK] the_there , but the the_divorce was invalid in england . in june 1901 ,_, he\n"
     ]
    }
   ],
   "source": [
    "# just one sentence \n",
    "\n",
    "test_sentence, test_labels, test_masks = text_pipeline('He married Mabel Scott in 1890, but they soon separated. \\\n",
    "Unable to get an English divorce, in 1900, he became the first celebrity to get one in Nevada, \\\n",
    "and remarried there, but the divorce was invalid in England. In June 1901, he was arrested for bigamy, \\\n",
    "and was convicted before the House of Lords, the last time a peer was convicted by the Lords.', 50)\n",
    "\n",
    "def print_outputs(test_sentence, test_labels, test_masks):\n",
    "    test_sentence = torch.tensor(test_sentence).long().cuda().view(1,-1)\n",
    "    #print(test_sentence)\n",
    "    test_mask_idxs = torch.ones_like(test_sentence).bool().cuda().view(1, -1)\n",
    "    #test_mask_idxs[:,40:] = False\n",
    "    #print(test_masks)\n",
    "    out = my_bert(test_sentence, test_mask_idxs)\n",
    "    #print(out.size())\n",
    "    out_sm = torch.softmax(out, dim=-1).argmax(dim=-1)\n",
    "    print(out)\n",
    "    #print(len(out_sm[0]))\n",
    "    \n",
    "    pprint(out_sm[0], test_labels, test_masks)\n",
    "\n",
    "    \n",
    "def pprint(predictions, labels, masks):\n",
    "    li = ['_'.join([vocab.vocab.get_itos()[pred], vocab.vocab.get_itos()[label]]) \n",
    "                    if mask \n",
    "                    else vocab.vocab.get_itos()[pred]\n",
    "                    for pred, label, mask in zip(predictions, torch.tensor(labels).long(), masks)]\n",
    "    li = [x for x in li if x != '[PAD]']\n",
    "    print(' '.join(li))\n",
    "\n",
    "#print([vocab.vocab.get_itos()[i] for i in test_sentence])\n",
    "#print([vocab.vocab.get_itos()[i] for i in out_sm[0]])\n",
    "#print([vocab.vocab.get_itos()[i] for i in torch.tensor(test_labels).long()])\n",
    "\n",
    "print_outputs(test_sentence, test_labels, test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test the first batch\n",
    "test_iter = to_map_style_dataset(dataset_test)  #Map-style dataset\n",
    "BATCH_SIZE = 32\n",
    "testloader = DataLoader(list(test_iter), batch_size=BATCH_SIZE, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ac560d05ad491ebc07cf5be34988a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8045/53046415.py:13: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
      "/tmp/ipykernel_8045/2302119576.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  for pred, label, mask in zip(predictions, torch.tensor(labels).long(), masks)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "fears '_for t n pension after talks unions jail_representing workers at turner the_newall say they are_are ' disappointed ' after talks the_with stricken parent firm federal the_mogul .\n",
      "1\n",
      "the race is on second private ,_team sets launch date for human spaceflight ( space . com ) space . com - toronto ,_, canada space_-- a [UNK] of ,_rocketeers competing for the #36 10 million ansari x prize , a contest [UNK] funded suborbital space flight , has officially announced the [UNK] date for its manned rocket .\n",
      "2\n",
      "ky ._. company wins grant to study peptides ( ap_ap ) ap - a company founded ritz_by a chemistry researcher at the university of_of louisville won a grant to develop the_a method the_of producing better peptides ,_, which are short chains of amino acids , the ,_building blocks of proteins .\n",
      "3\n",
      "prediction ,_unit helps forecast wildfires ( ap_ap ) ap - burden_it ' s ,_barely dawn when mike fitzpatrick starts his shift with a blur of colorful the_maps , figures and endless charts , but already_already he ,_knows what the day will ,_bring . lightning will strike in places the_he expects . winds will pick up , moist places will dry and flames ._will roar .\n",
      "4\n",
      "calif . s_aims to limit [UNK] smog (_( ap ) ap_ap - southern california ' s [UNK] agency the_went after ,_emissions the_of the bovine_bovine variety friday , adopting the nation ' erases_s first rules to reduce air pollution from dairy cow manure .\n",
      "5\n",
      "open letter against british_british copyright [UNK] of_in schools the british department for education and skills_skills ( [UNK] ) recently launched the_a music manifesto campaign , with of_the [UNK] intention of educating the_the next generation of british musicians . unfortunately ,_, the_they also teamed ,_up with the music industry ( emi , and ,_various ,_artists ) to make this popular . emi has apparently negotiated their end well , so that jail_children in our schools will now be [UNK] about ,_the [UNK] of downloading music . the ignorance and [UNK] of this got to ,_me a little , so i wrote\n",
      "6\n",
      "[UNK] ,_the war on terrorism [UNK] jaschan , self-confessed author of the netsky and sasser ,_viruses , the_[UNK] for 70 percent of virus infections in 2004 , according to a [UNK] roundup published_published wednesday by antivirus company sophos . \\\\the 18-year-old jaschan was taken into custody in germany in may by police [UNK] he had ,_admitted programming both the netsky and_and hollen_sasser worms , [UNK] at microsoft confirmed . ( a microsoft antivirus chilly program led to [UNK] ,_' s arrest . ) during the five months preceding jaschan ' s ,_capture ,_, [UNK] at least 25 ._variants of netsky\n",
      "7\n",
      "[UNK] [UNK] , [UNK] , key distribution , and the_bloom filters [UNK] and bloom filters have a lot of interesting properties for [UNK] and ._whitelist distribution . \\\\i think we can go ._one level higher though and include [UNK] [UNK] distribution in ._the [UNK] file for simple [UNK] ._based [UNK] . \\\\what schuler_if we used [UNK] and included the [UNK] key fingerprint_fingerprint ( s ) for identities ? , could mean a ._lot . ._you include the [UNK] key fingerprints within the [UNK] of your direct friends and then include a bloom e-christmas of the [UNK] [UNK] of your entire\n",
      "8\n",
      "e-mail scam targets police chief [UNK] ,_police warns_warns about phishing after its of_fraud squad chief was targeted .\n",
      "9\n",
      "card fraud unit nets ,_36 , 000_000 cards in its first dorsey_two years , the uk ' s dedicated card the_fraud unit , has recovered 36 ,_, 000 stolen ,_cards and 171 arrests - and ,_estimates it saved 65m .\n",
      "10\n",
      "group to propose new high-speed wireless format los angeles ( reuters_reuters ) - the_a group_group of technology companies ,_including texas instruments inc . &lt txn . n&gt , stmicroelectronics &lt ) . ._[UNK] and broadcom corp . &lt brcm . o&gt , on thursday said sachnin_they will propose a new wireless networking standard up to 10 times the ._speed of the current generation .\n",
      "11\n",
      "apple launches graphics software , video bundle los angeles ( reuters ) -_- apple apple_computer inc . &lt aapl nasd_. o&gt on tuesday_tuesday began of_shipping a new program to_designed to_to let users create_create real-time motion graphics to_and unveiled to_a discount video-editing software bundle featuring its flagship final cut pro software ._.\n",
      "12\n",
      "dutch retailer beats apple to to_local download market amsterdam ( reuters ) - free record shop , a of_dutch music_music retail the_chain , beat apple computer inc . to market on tuesday the_with the launch of a ,_new download the_service in europe ' s ._latest battleground for digital song services .\n",
      "13\n",
      "super ant to_colony to_hits australia a giant 100km colony of ants the_which has to_been discovered in_in melbourne , australia , the_could threaten used_local insect the_species .\n",
      "14\n",
      "[UNK] unite dolphin to_groups dolphin groups the_, or pods , rely on [UNK] to keep them from_from collapsing ,_, scientists claim .\n",
      "15\n",
      "teenage u_t . rex ' s monster growth tyrannosaurus rex achieved its massive size due to an enormous_enormous growth spurt to_during its to_adolescent to_years .\n",
      "16\n",
      "scientists discover ganymede has a_a [UNK] interior jet propulsion lab -- scientists have discovered irregular nuzzo beneath the icy of_surface of jupiter ' s_s largest moon , ganymede . these irregular masses may be rock formations , ,_supported by ganymede ' s icy shell for billions of years . . .\n",
      "17\n",
      "tx_mars to_rovers relay images through mars express european space agency -- esas the_mars express has relayed pictures from one of nasa #39_' s mars rovers for the of_first time ,_, the_as part of a set of interplanetary networking ,_demonstrations . the_the demonstrations pave the way for the_future mars missions to draw on joint interplanetary networking capabilities . . .\n",
      "18\n",
      "rocking the cradle to_of life when did life begin ? one [UNK] of_clue stems from the fossil records in western australia , although whether these layered being_[UNK] ,_are the_biological or chemical has spawned a spirited debate . oxford researcher , [UNK] the_[UNK] , describes some ,_of the issues in contention ._.\n",
      "19\n",
      "storage , servers to_bruise hp earnings update earnings per share rise compared with ,_a year ago a_, but company_company misses analysts ' expectations by a long shot .\n",
      "20\n",
      "ibm to hire even more new workers by the end of the_the year , the computing giant plans to have its biggest headcount the_since 1991 .\n",
      "21\n",
      "sun '_' s #39_looking glass provides 3d view developers get early ,_code for new operating system ' skin '_' still being crafted ._.\n",
      "22\n",
      "ibm chips may someday_someday heal themselves new technology applies electrical fuses to help the_identify and the_repair faults .\n",
      "23\n",
      "some people not eligible_eligible to the_get in the_on google ipo google has billed its ipo as a way for everyday people buyouts_to get in on the process , denying wall street the usual stranglehold it ' s ,_had on ipos . public bidding , a minimum of the_just five shares ,_, an open process with 28 underwriters - all this pointed to a new level s_of public ._participation . but this isn ' t the case .\n",
      "24\n",
      "rivals try to_to turn tables on charles schwab by michael liedtke san francisco ( ap ) -- the_with its low prices and [UNK] attitude , discount stock broker charles ._schwab corp . ( sch ) represented an annoying stone in wall street ' s the_[UNK] shoes for decades . . .\n",
      "25\n",
      "news sluggish movement to_on power_power grid cyber security industry the_cyber security standards fail to reach some of the most vulnerable components of the power grid . \\\n",
      "26\n",
      "giddy phelps touches gold for the_first time_time michael phelps won the gold medal in the 400 individual medley and set a world record in a time of 4 minutes the_8 . 26 seconds .\n",
      "27\n",
      "tougher '_rules won ' t soften law ' s game foxborough -- looking_looking at his [UNK] developed upper body , with huge the_biceps and hardly an ounce of the_fat , it ' s ,_easy ,_to see why ty law ,_, arguably the best cornerback 402-2_in football , chooses physical_physical ,_play over finesse . that ' s not to imply that he ' s lacking a finesse component ._, the_because he can shut down his side of the field much as deion sanders . . .\n",
      "28\n",
      "[UNK] doesn #39_' t appear ready to hit_hit the next level with the weeks dwindling ,_until jason varitek enters free agency , the red ,_sox continue to_to carefully monitor kelly ,_[UNK] , their catcher ,_of knocked_the future , in his ,_climb ,_toward the majors . the ,_sox like most of what they have seen at triple a pawtucket from [UNK] , though it remains highly uncertain whether he can make the adjustments at the ._plate . . .\n",
      "29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mighty to_ortiz makes sure_sure sox can ,_rest easy just imagine what david ortiz could do on a ,_good ,_night ' s rest . ortiz spent ,_the night before last with his baby boy , d ' angelo the_, the_who is barely 1 month old . he had planned on attending the red sox ' family day at ,_fenway ,_park yesterday morning , but he had to ,_sleep in . the_after all , ortiz had a son at home , and he ._. ._. .\n",
      "30\n",
      "they ' ve to_caught his eye in quot helping themselves , quot the_ricky bryant , [UNK] [UNK] dance_, the_michael jennings , and_and the_david the_patten did nothing friday the_night to make bill belichick ' s decision on what to do with his ,_receivers any easier .\n",
      "31\n",
      "indians mount charge the cleveland indians pulled within one game of the al central lead by beating the minnesota twins , 7-1 , saturday night with the_home runs by_by travis hafner and victor martinez .\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "\n",
    "for i, (x, y, mask_idxs) in tqdm(enumerate(testloader), total=len(dataset_test)//BATCH_SIZE):\n",
    "    \n",
    "    #optimizer.zero_grad()\n",
    "    out = my_bert(x, mask_idxs)\n",
    "\n",
    "    out_sm = torch.softmax(out, dim=-1).argmax(dim=-1)\n",
    "    \n",
    "    for j in range(BATCH_SIZE):\n",
    "        print(j)\n",
    "        pprint(out_sm[j], y[j], mask_idxs[j])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test against random loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0521db38d8584412ab4970dfdf8566f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8045/53046415.py:13: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  label_list = torch.tensor(label_list, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss from random network: 2711.031\n"
     ]
    }
   ],
   "source": [
    "### test the model on a random init version of the network\n",
    "random_network = MyBERT(embedding_dim=EMBED_DIM, \n",
    "               heads=NUM_HEADS, \n",
    "               seq_length=SEQ_LENGTH,\n",
    "               vocab_size=VOCAB_SIZE,\n",
    "               device=DEVICE,\n",
    "               depth=NUM_LAYERS).to(DEVICE)\n",
    "\n",
    "random_network.eval()\n",
    "\n",
    "random_test_loss = 0\n",
    "for i, (x, y, masks_idxs) in tqdm(enumerate(testloader), total=len(dataset_test)//BATCH_SIZE):\n",
    "\n",
    "        #if i > 50: break\n",
    "        x, y, masks_idxs = x.to(DEVICE), y.to(DEVICE), masks_idxs.to(DEVICE)\n",
    "\n",
    "        out = random_network(x, masks_idxs)\n",
    "        out = torch.swapaxes(out, 1, 2)\n",
    "        \n",
    "        loss = criterion(out, y)\n",
    "        random_test_loss += loss.item()\n",
    "        \n",
    "print(\"test loss from random network: {:.03f}\".format(random_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6702552f7243a799a1890e3f739cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8045/53046415.py:13: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  label_list = torch.tensor(label_list, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss from random network: 87.116\n"
     ]
    }
   ],
   "source": [
    "### load model\n",
    "\n",
    "trained_model = MyBERT(embedding_dim=EMBED_DIM, \n",
    "               heads=NUM_HEADS, \n",
    "               seq_length=SEQ_LENGTH,\n",
    "               vocab_size=VOCAB_SIZE,\n",
    "               device=DEVICE,\n",
    "               depth=NUM_LAYERS).to(DEVICE)\n",
    "\n",
    "trained_model.load_state_dict(torch.load(BERT_PATH))\n",
    "trained_model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "for i, (x, y, masks_idxs) in tqdm(enumerate(testloader), total=len(dataset_test)//BATCH_SIZE):\n",
    "\n",
    "        #if i > 50: break\n",
    "        x, y, masks_idxs = x.to(DEVICE), y.to(DEVICE), masks_idxs.to(DEVICE)\n",
    "\n",
    "        out = trained_model(x, masks_idxs)\n",
    "        out = torch.swapaxes(out, 1, 2)\n",
    "        \n",
    "        loss = criterion(out, y)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "print(\"test loss from random network: {:.03f}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
